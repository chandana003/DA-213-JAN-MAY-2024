{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcGq1dF0iWju"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install these if they were not installed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\91834\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\91834\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\91834\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\91834\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91834\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\91834\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91834\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\91834\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\91834\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\91834\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91834\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from string import ascii_lowercase\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8KPqb7_vC59"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "96Z6cXAZiaAY",
    "outputId": "6f4935e6-9bef-4b02-c961-c2e8ff3dedee"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('jigsaw-toxic-comment-train-processed-seqlen128.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "yTbZDdkc1lxE",
    "outputId": "0fde7670-bf28-40cd-ad0b-a307fcb778e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>input_word_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>all_segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 27746, 31609, 11809, 24781, 10105, 70971...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 141, 112, 56237, 10874, 106, 10357, 1825...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 35936, 10817, 117, 146, 112, 181, 30181,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 107, 15946, 146, 10944, 112, 188, 13086,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 11065, 117, 52523, 117, 10301, 15127, 51...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 107, 12845, 84558, 74874, 10188, 10911, ...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 32992, 10858, 62828, 93089, 11733, 24093...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 13554, 109995, 33269, 10147, 10114, 1010...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 98314, 12277, 10105, 12307, 112, 10446, ...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 70308, 10426, 10135, 10531, 20036, 10111...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "5             0        0       0       0              0   \n",
       "6             1        1       0       1              0   \n",
       "7             0        0       0       0              0   \n",
       "8             0        0       0       0              0   \n",
       "9             0        0       0       0              0   \n",
       "\n",
       "                                      input_word_ids  \\\n",
       "0  (101, 27746, 31609, 11809, 24781, 10105, 70971...   \n",
       "1  (101, 141, 112, 56237, 10874, 106, 10357, 1825...   \n",
       "2  (101, 35936, 10817, 117, 146, 112, 181, 30181,...   \n",
       "3  (101, 107, 15946, 146, 10944, 112, 188, 13086,...   \n",
       "4  (101, 11065, 117, 52523, 117, 10301, 15127, 51...   \n",
       "5  (101, 107, 12845, 84558, 74874, 10188, 10911, ...   \n",
       "6  (101, 32992, 10858, 62828, 93089, 11733, 24093...   \n",
       "7  (101, 13554, 109995, 33269, 10147, 10114, 1010...   \n",
       "8  (101, 98314, 12277, 10105, 12307, 112, 10446, ...   \n",
       "9  (101, 70308, 10426, 10135, 10531, 20036, 10111...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "5  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "7  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "8  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "9  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      all_segment_id  \n",
       "0  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hAcigh3Sh3"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsOBGKJI4aHT"
   },
   "source": [
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZzAnL7ly1epE",
    "outputId": "b7e5c03e-07cc-49cd-bdc8-510f25c96749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                False\n",
       "comment_text      False\n",
       "toxic             False\n",
       "severe_toxic      False\n",
       "obscene           False\n",
       "threat            False\n",
       "insult            False\n",
       "identity_hate     False\n",
       "input_word_ids    False\n",
       "input_mask        False\n",
       "all_segment_id    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TYZ93LlRbwRB"
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDWRHguXxOF_"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z-4hq4jyhBw"
   },
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4k-vI8DbqiP"
   },
   "source": [
    "* Removing Characters in between Text\n",
    "* Removing Repeated Characters\n",
    "* Converting data to lower-case\n",
    "* Removing Numbers from the data\n",
    "* Remove Punctuation\n",
    "* Remove Whitespaces\n",
    "* Removing spaces in between words\n",
    "* Removing \"\\n\"\n",
    "* Remove Non-english characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jglnA7LEbpoA"
   },
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu', 'st*u'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    ' fucking ':\n",
    "        [\n",
    "            'f*$%-ing'\n",
    "        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G32H2jt-J_vz"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4RetJ1vgswOG"
   },
   "outputs": [],
   "source": [
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "    text=text.lower()\n",
    "    \n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "        for pat in patterns:\n",
    "            text=str(text).replace(pat, target)\n",
    "    \n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "    text = str(text).replace(\"\\n\", \" \")\n",
    "    #Ascii Removal\n",
    "    text = re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "    #URL removal\n",
    "    text = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
    "    #Remove all the special characters \n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub('[0-9]',\"\",text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    #Date Removal \n",
    "    text = re.sub('(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{2}\\s\\d{4}', ' ', text)\n",
    "    text = re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwyO_GcPuPE_"
   },
   "source": [
    "Cleaning Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Cak-Q0fl0qyb",
    "outputId": "241c5593-10ff-4ce0-fb68-aaa573760b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daww he matches this background colour im seemingly stuck with thanks talk january utc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(clean_text)\n",
    "train['comment_text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqRVmH1FRWpL"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vnhCAXkKUF9i"
   },
   "outputs": [],
   "source": [
    "comments_train=train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WwhTNF3pW7Hp"
   },
   "outputs": [],
   "source": [
    "comments_train=list(comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IuWMc1xqRVqV"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fi-vYUcPoi-a"
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "#Pos Tagging-Noun,verb,adjective,adverb\n",
    "def lemma(text):\n",
    "    output=\"\"\n",
    "    \n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "       output=output + \" \" + word4\n",
    "  \n",
    "    return str(output.strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WK9azHjSrbic",
    "outputId": "53c89e46-74f8-49a4-825a-b06bbcb8b132"
   },
   "outputs": [],
   "source": [
    "lemmatized_train_data = [] \n",
    "\n",
    "for line in comments_train:\n",
    "    newline = lemma(line)\n",
    "    lemmatized_train_data.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i agree with you about graemels intention he be a revert nazi and wikipedia ha a way of protect against this report him to the wprr threerevertrule notice board post by report the revert nazi graemel ha show himselfherself to be a revert nazi this be unacceptable on a site that be make by it user and not it admins if you feel you have be unjustly revert more than time over a hour period please report himher to the threerevertnoticeboard wprr it be time to take back what be ours'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_train_data[152458]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZaP8BH2UG0N"
   },
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vNKjE4Vq8kpG"
   },
   "outputs": [],
   "source": [
    "stopword_list = STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_oqV08mMWmk"
   },
   "source": [
    "Adding Single and Dual to STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "go7RVS52I1US"
   },
   "outputs": [],
   "source": [
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "dual_alpha_list=[]\n",
    "for s in iter_all_strings():\n",
    "    dual_alpha_list.append(s)\n",
    "    if s == 'zz':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7_x3bQ4MIGVK"
   },
   "outputs": [],
   "source": [
    "dual_alpha_list.remove('i')\n",
    "dual_alpha_list.remove('a')\n",
    "dual_alpha_list.remove('am')\n",
    "dual_alpha_list.remove('an')\n",
    "dual_alpha_list.remove('as')\n",
    "dual_alpha_list.remove('at')\n",
    "dual_alpha_list.remove('be')\n",
    "dual_alpha_list.remove('by')\n",
    "dual_alpha_list.remove('do')\n",
    "dual_alpha_list.remove('go')\n",
    "dual_alpha_list.remove('he')\n",
    "dual_alpha_list.remove('hi')\n",
    "dual_alpha_list.remove('if')\n",
    "dual_alpha_list.remove('is')\n",
    "dual_alpha_list.remove('in')\n",
    "dual_alpha_list.remove('me')\n",
    "dual_alpha_list.remove('my')\n",
    "dual_alpha_list.remove('no')\n",
    "dual_alpha_list.remove('of')\n",
    "dual_alpha_list.remove('on')\n",
    "dual_alpha_list.remove('or')\n",
    "dual_alpha_list.remove('ok')\n",
    "dual_alpha_list.remove('so')\n",
    "dual_alpha_list.remove('to')\n",
    "dual_alpha_list.remove('up')\n",
    "dual_alpha_list.remove('us')\n",
    "dual_alpha_list.remove('we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "H11kkMtXMyct",
    "outputId": "7ef93dc1-26fc-46ec-fc4f-468687985d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for letter in dual_alpha_list:\n",
    "    stopword_list.add(letter)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti67XuLCNnsL"
   },
   "source": [
    "Checking for other words that we may need in STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z4lBqjcBaDVK"
   },
   "outputs": [],
   "source": [
    "def search_stopwords(data):\n",
    "    output=\"\"\n",
    "    data=data.split(\" \")\n",
    "    for word in data:\n",
    "        if not word in stopword_list:\n",
    "            output=output+\" \"+word \n",
    "\n",
    "    return str(output.strip())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "eef94b9a14a44f1b9adaebd911d53402"
     ]
    },
    "id": "xn21RMeIbQti",
    "outputId": "41e7a7ef-f532-4cfc-bdf9-1a770163f921"
   },
   "outputs": [],
   "source": [
    "processed_train_data = [] \n",
    "\n",
    "for line in lemmatized_train_data: \n",
    "    processed_train_data.append(search_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeScalvZDEdD"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SkK1P-CdX_0N"
   },
   "outputs": [],
   "source": [
    "max_features=100000      \n",
    "maxpadlen = 200          \n",
    "val_split = 0.2      \n",
    "embedding_dim_fasttext = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIeovLIr6aAo"
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qaA52PlK4xnV"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(processed_train_data))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223549"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pkSXj5sT7jbR",
    "outputId": "307a0e89-169a-4c96-a0e8-35573bdf18f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263878"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "l = len(word_index)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfDwmzPj8TJf"
   },
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gEmacO337twa"
   },
   "outputs": [],
   "source": [
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U6dAeWAnM4RL",
    "outputId": "c42c0ed1-c46d-437f-c8e2-7b89aa5319a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: \n",
      " [ 384    3  460 3499 8445  571   55 1485  108 4307 1713  330   43  870\n",
      " 7985 1941    9   25  176    6    2 2045    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "One hot label: \n",
      " [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentences: \\n', X_t[0])\n",
    "print('One hot label: \\n', y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XtZD1j9BCrsG"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(X_t.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "k51hkGqOnZLB",
    "outputId": "1641df61-cac1-4885-d7cd-6cb942c2d9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are : \n",
      " [[  60  320  191 ...    0    0    0]\n",
      " [  70   13  761 ...    0    0    0]\n",
      " [  12  127   24 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [ 260    2    3 ...    0    0    0]\n",
      " [9139 1300  133 ...    0    0    0]]\n",
      "labels are: \n",
      " [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ...\n",
      " [1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_t = X_t[indices]\n",
    "labels = y[indices]\n",
    "print('Features are :','\\n',X_t)\n",
    "print('labels are:','\\n',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWn-SkJCCswd"
   },
   "source": [
    "### Splitting data into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "B5hOSGJVb7o4"
   },
   "outputs": [],
   "source": [
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178840"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimenting without using in built functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip below blocks of code as the memomy is unable to allocate memory to 3D array of required size for this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 68.7 TiB for an array with shape (178840, 200, 263878) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index)  \u001b[38;5;66;03m# Length of the vocabulary\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initializing the 3D array with zeros\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m x_train_3D \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxpadlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Function to perform one-hot encoding\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encode\u001b[39m(sequences, vocab_size, max_length):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Create a 3D array of zeros\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 68.7 TiB for an array with shape (178840, 200, 263878) and data type float64"
     ]
    }
   ],
   "source": [
    "# Initializing parameters\n",
    "num_samples = len(x_train)\n",
    "maxpadlen = 200  # Same as defined previously\n",
    "vocab_size = len(word_index)  # Length of the vocabulary\n",
    "\n",
    "# Initializing the 3D array with zeros\n",
    "x_train_3D = np.zeros((num_samples, maxpadlen, vocab_size))\n",
    "\n",
    "# Function to perform one-hot encoding\n",
    "def one_hot_encode(sequences, vocab_size, max_length):\n",
    "    # Create a 3D array of zeros\n",
    "    one_hot_encoded = np.zeros((len(sequences), max_length, vocab_size))\n",
    "    # Iterate through each sequence\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        # Iterate through each word index in the sequence\n",
    "        for j, word_index in enumerate(sequence):\n",
    "            # Ensure the word index is within the vocabulary size\n",
    "            if word_index < vocab_size:\n",
    "                # Set the appropriate position in the vector to 1\n",
    "                one_hot_encoded[i, j, word_index] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "# One-hot encoding the x_train data\n",
    "x_train_3D = one_hot_encode(x_train, vocab_size, maxpadlen)\n",
    "\n",
    "# Checking the shape of the resulting 3D array\n",
    "x_train_3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(x,n_x):\n",
    "    size , T_x = x.shape\n",
    "    \n",
    "    one_hot_x = np.zeros((n_x,size,T_x))\n",
    "    \n",
    "    for k in range(0,T_x):\n",
    "        j = 0\n",
    "        for i in x[:,k]:\n",
    "            one_hot_x[i,j,k] = 1\n",
    "            j = j+1\n",
    "    \n",
    "    return one_hot_x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "    ### START CODE HERE ### (approx. 4 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v, s\n",
    "\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
    "    bi = parameters[\"bi\"] # (notice the variable name)\n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt \n",
    "    concat = np.concatenate((a_prev,xt),axis=0)\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given \n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c_next = ft*c_prev + it*cct\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    a_next = ot*np.tanh(c_next)\n",
    "    \n",
    "    # Compute prediction of the LSTM cell \n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
    "    n_x, m, T_x = np.shape(x)\n",
    "    n_y, n_a = np.shape(Wy)\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:,:,t]\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the next cell state \n",
    "        c[:,:,t]  = c_next\n",
    "        # Save the value of the prediction in y \n",
    "        y[:,:,t] = yt\n",
    "        # Append the cache into caches \n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from xt's and a_next's shape\n",
    "    n_x, m = xt.shape \n",
    "    n_a, m = a_next.shape \n",
    "    \n",
    "    # Compute gates related derivatives\n",
    "   \n",
    "    dit = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * cct * (1 - it) * it\n",
    "    dft = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * c_prev * ft * (1 - ft)\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * it * (1 - cct ** 2)\n",
    "\n",
    "    # Compute parameters related derivatives. Use equations \n",
    "    dWf = np.dot(dft,np.concatenate((a_prev, xt), axis=0).T) # or use np.dot(dft, np.hstack([a_prev.T, xt.T]))\n",
    "    dWi = np.dot(dit,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWc = np.dot(dcct,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True) \n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True) \n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)  \n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input.. \n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit)+np.dot(parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot) \n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next \n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot) \n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes \n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes \n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWi = np.zeros((n_a, n_a + n_x))\n",
    "    dWc = np.zeros((n_a, n_a + n_x))\n",
    "    dWo = np.zeros((n_a, n_a + n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = lstm_cell_backward(da[:,:,t] + da_prevt, dc_prevt, caches[t])\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = gradients[\"dxt\"]\n",
    "        dWf += gradients[\"dWf\"]\n",
    "        dWi += gradients[\"dWi\"]\n",
    "        dWc += gradients[\"dWc\"]\n",
    "        dWo += gradients[\"dWo\"]\n",
    "        dbf += gradients[\"dbf\"]\n",
    "        dbi += gradients[\"dbi\"]\n",
    "        dbc += gradients[\"dbc\"]\n",
    "        dbo += gradients[\"dbo\"]\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = gradients[\"da_prev\"]\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_a, n_y):\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    parameters = {}\n",
    "    parameters['Wf'] = np.random.randn(n_a, n_a + n_x) * 0.01\n",
    "    parameters['Wi'] = np.random.randn(n_a, n_a + n_x) * 0.01\n",
    "    parameters['Wc'] = np.random.randn(n_a, n_a + n_x) * 0.01\n",
    "    parameters['Wo'] = np.random.randn(n_a, n_a + n_x) * 0.01\n",
    "    parameters['Wy'] = np.random.randn(n_y, n_a) * 0.01\n",
    "    \n",
    "    parameters['bf'] = np.zeros((n_a, 1))\n",
    "    parameters['bi'] = np.zeros((n_a, 1))\n",
    "    parameters['bc'] = np.zeros((n_a, 1))\n",
    "    parameters['bo'] = np.zeros((n_a, 1))\n",
    "    parameters['by'] = np.zeros((n_y, 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(x,y,n_x):\n",
    "    \n",
    "    #retreiving dimensions from data\n",
    "    m ,T_x = x.shape\n",
    "    n_y = y.shape[0]\n",
    "    #initializing n_a\n",
    "    n_a = 4\n",
    "    \n",
    "    #converting input matrix into \n",
    "    \n",
    "    #initializing parameters\n",
    "    parameters = initialize_parameters(n_x,n_a,n_y)\n",
    "    \n",
    "    #initializing a0 \n",
    "    a0 = np.random.randn(n_a,m) * 0.01\n",
    "    \n",
    "    #forward propagation\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimenting using in built functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=64))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation='sigmoid'))  # 6 output classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with early stopping and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 233ms/step - accuracy: 0.9947 - loss: 0.1128 - val_accuracy: 0.9952 - val_loss: 0.1108 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 235ms/step - accuracy: 0.9950 - loss: 0.1234 - val_accuracy: 0.9952 - val_loss: 0.0776 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9946 - loss: 0.0698 - val_accuracy: 0.9952 - val_loss: 0.0586 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 237ms/step - accuracy: 0.9950 - loss: 0.0555 - val_accuracy: 0.9952 - val_loss: 0.0573 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 238ms/step - accuracy: 0.9948 - loss: 0.0484 - val_accuracy: 0.9952 - val_loss: 0.0581 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=64,  # Increased batch size for better training efficiency\n",
    "    epochs=5,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"optimized_lstm_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict toxicity levels for a given comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment):\n",
    "    # Clean and preprocess the comment\n",
    "    cleaned_comment = clean_text(comment)\n",
    "    lemmatized_comment = lemma(cleaned_comment)\n",
    "    tokenized_comment = tokenizer.texts_to_sequences([lemmatized_comment])\n",
    "    padded_comment = pad_sequences(tokenized_comment, maxlen=maxpadlen, padding='post')\n",
    "    \n",
    "    # Predict using the model\n",
    "    prediction = model.predict(padded_comment)\n",
    "    \n",
    "    # Define classes\n",
    "    classes = ['Toxic', 'Severe Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate']\n",
    "    \n",
    "    # Format the output\n",
    "    output = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        output[cls] = f\"{prediction[0][i] * 100:.0f}%\"\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Toxic': '67%',\n",
       " 'Severe Toxic': '1%',\n",
       " 'Obscene': '29%',\n",
       " 'Threat': '2%',\n",
       " 'Insult': '30%',\n",
       " 'Identity Hate': '4%'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity('go jump off a bridge jerk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Toxic': '15%',\n",
       " 'Severe Toxic': '0%',\n",
       " 'Obscene': '3%',\n",
       " 'Threat': '1%',\n",
       " 'Insult': '4%',\n",
       " 'Identity Hate': '1%'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity('i will kill you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Toxic': '3%',\n",
       " 'Severe Toxic': '0%',\n",
       " 'Obscene': '0%',\n",
       " 'Threat': '0%',\n",
       " 'Insult': '1%',\n",
       " 'Identity Hate': '0%'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity('Hello, How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Toxic': '93%',\n",
       " 'Severe Toxic': '8%',\n",
       " 'Obscene': '78%',\n",
       " 'Threat': '3%',\n",
       " 'Insult': '65%',\n",
       " 'Identity Hate': '11%'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity('fuck ofF!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Toxic': '94%',\n",
       " 'Severe Toxic': '8%',\n",
       " 'Obscene': '80%',\n",
       " 'Threat': '3%',\n",
       " 'Insult': '66%',\n",
       " 'Identity Hate': '11%'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity('get the fuck away from me @sshole!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wcGq1dF0iWju",
    "z8KPqb7_vC59",
    "D8hAcigh3Sh3",
    "NDWRHguXxOF_",
    "CeScalvZDEdD",
    "7wv1IdwZk_qG",
    "bXx4IOYn4_XB",
    "kJJJs5txVS8S",
    "7HkAldq1IJcr"
   ],
   "name": "LSTM_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
